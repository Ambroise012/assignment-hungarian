{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b8c6c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost (unweighted): 14.0\n",
      "Cost (weighted):   5.563000000000001\n",
      "Written: assignment_student_unweighted.csv , assignment_project_unweighted.csv , assignment_student_weighted.csv , assignment_project_weighted.csv\n",
      "\n",
      "=== Unweighted — Students ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student</th>\n",
       "      <th>project_id</th>\n",
       "      <th>choice_rank</th>\n",
       "      <th>choice_weight</th>\n",
       "      <th>project_label</th>\n",
       "      <th>initial_choice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aaron HUMBERT</td>\n",
       "      <td>P1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Data Cleaning</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adam FOURNIER</td>\n",
       "      <td>P5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Time Series Forecasting</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Agathe DUPUIS</td>\n",
       "      <td>P10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Computer Vision</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alice MARTIN</td>\n",
       "      <td>P1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.133</td>\n",
       "      <td>Data Cleaning</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amandine RENARD</td>\n",
       "      <td>P9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Web Analytics</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ambre FABRE</td>\n",
       "      <td>P4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>NLP Chatbot</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Anaïs LÉFÈVRE</td>\n",
       "      <td>P4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>NLP Chatbot</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Antoine PERROT</td>\n",
       "      <td>P2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Recommender System</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Arthur DAVID</td>\n",
       "      <td>P7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Reinforcement Learning</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Aya MARTINEZ</td>\n",
       "      <td>P10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Computer Vision</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Baptiste LEGRAND</td>\n",
       "      <td>P11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Graph Mining</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Camille SIMON</td>\n",
       "      <td>P5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.133</td>\n",
       "      <td>Time Series Forecasting</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Chloé ROBERT</td>\n",
       "      <td>P4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.167</td>\n",
       "      <td>NLP Chatbot</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Clara CARON</td>\n",
       "      <td>P13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Edge AI</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Clément MARCHAND</td>\n",
       "      <td>P13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Edge AI</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Eliott CHARPENTIER</td>\n",
       "      <td>P1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Data Cleaning</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Emma DUBOIS</td>\n",
       "      <td>P2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Recommender System</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Enzo PAUL</td>\n",
       "      <td>P11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Graph Mining</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Ethan GIRARD</td>\n",
       "      <td>P2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Recommender System</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Gabriel PETIT</td>\n",
       "      <td>P4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.167</td>\n",
       "      <td>NLP Chatbot</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Hugo THOMAS</td>\n",
       "      <td>P2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Recommender System</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Inès GARCIA</td>\n",
       "      <td>P7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Reinforcement Learning</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Iris BLANCHARD</td>\n",
       "      <td>P3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Image Classification</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Jade COLIN</td>\n",
       "      <td>P11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Graph Mining</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Jeanne BERTRAND</td>\n",
       "      <td>P5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Time Series Forecasting</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Jules LAURENT</td>\n",
       "      <td>P7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Reinforcement Learning</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Justine LOPES</td>\n",
       "      <td>P8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Optimization Engine</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Lila GAUTIER</td>\n",
       "      <td>P12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>AutoML Pipeline</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Lina MOREL</td>\n",
       "      <td>P1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Data Cleaning</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Louis DURAND</td>\n",
       "      <td>P3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Image Classification</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Lucas BERNARD</td>\n",
       "      <td>P1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.133</td>\n",
       "      <td>Data Cleaning</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Lucie NOËL</td>\n",
       "      <td>P1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Data Cleaning</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Léa RICHARD</td>\n",
       "      <td>P3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Image Classification</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Manon LEROY</td>\n",
       "      <td>P3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Image Classification</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Marion DUMAS</td>\n",
       "      <td>P8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.133</td>\n",
       "      <td>Optimization Engine</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Marius MATHIEU</td>\n",
       "      <td>P2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Recommender System</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Maxime MULLER</td>\n",
       "      <td>P4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>NLP Chatbot</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Maël ANDRÉ</td>\n",
       "      <td>P3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Image Classification</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Maëlle BOUCHET</td>\n",
       "      <td>P12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>AutoML Pipeline</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Mila DUPONT</td>\n",
       "      <td>P6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Anomaly Detection</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Nathan MOREAU</td>\n",
       "      <td>P5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.133</td>\n",
       "      <td>Time Series Forecasting</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Nina BARBIER</td>\n",
       "      <td>P2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Recommender System</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Nino OLIVIER</td>\n",
       "      <td>P6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Anomaly Detection</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Noa NAVARRO</td>\n",
       "      <td>P9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Web Analytics</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Noé ROUX</td>\n",
       "      <td>P5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Time Series Forecasting</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Océane GUERIN</td>\n",
       "      <td>P13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Edge AI</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Oscar BOYER</td>\n",
       "      <td>P10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Computer Vision</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Pauline GAUTHIER</td>\n",
       "      <td>P6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.133</td>\n",
       "      <td>Anomaly Detection</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Raphaël MERCIER</td>\n",
       "      <td>P6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.133</td>\n",
       "      <td>Anomaly Detection</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Romane GONZALEZ</td>\n",
       "      <td>P4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.133</td>\n",
       "      <td>NLP Chatbot</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Sacha FRANÇOIS</td>\n",
       "      <td>P9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Web Analytics</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Salomé COUSIN</td>\n",
       "      <td>P6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Anomaly Detection</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Sarah LEFEBVRE</td>\n",
       "      <td>P7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Reinforcement Learning</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Théo FONTANA</td>\n",
       "      <td>P12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>AutoML Pipeline</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Timéo PIRES</td>\n",
       "      <td>P6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.133</td>\n",
       "      <td>Anomaly Detection</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Tom MICHEL</td>\n",
       "      <td>P7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Reinforcement Learning</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Valentin CHARLES</td>\n",
       "      <td>P3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Image Classification</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Victor LAMBERT</td>\n",
       "      <td>P8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.133</td>\n",
       "      <td>Optimization Engine</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Yanis RENAUD</td>\n",
       "      <td>P8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Optimization Engine</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Zoé VINCENT</td>\n",
       "      <td>P5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Time Series Forecasting</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Élise BONNET</td>\n",
       "      <td>P8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Optimization Engine</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Émile REY</td>\n",
       "      <td>P7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Reinforcement Learning</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               student project_id  choice_rank  choice_weight  \\\n",
       "0        Aaron HUMBERT         P1            1          0.100   \n",
       "1        Adam FOURNIER         P5            1          0.100   \n",
       "2        Agathe DUPUIS        P10            1          0.100   \n",
       "3         Alice MARTIN         P1            2          0.133   \n",
       "4      Amandine RENARD         P9            1          0.100   \n",
       "5          Ambre FABRE         P4            1          0.100   \n",
       "6        Anaïs LÉFÈVRE         P4            1          0.100   \n",
       "7       Antoine PERROT         P2            1          0.100   \n",
       "8         Arthur DAVID         P7            1          0.100   \n",
       "9         Aya MARTINEZ        P10            1          0.100   \n",
       "10    Baptiste LEGRAND        P11            1          0.100   \n",
       "11       Camille SIMON         P5            2          0.133   \n",
       "12        Chloé ROBERT         P4            3          0.167   \n",
       "13         Clara CARON        P13            1          0.100   \n",
       "14    Clément MARCHAND        P13            1          0.100   \n",
       "15  Eliott CHARPENTIER         P1            1          0.100   \n",
       "16         Emma DUBOIS         P2            1          0.100   \n",
       "17           Enzo PAUL        P11            1          0.100   \n",
       "18        Ethan GIRARD         P2            1          0.100   \n",
       "19       Gabriel PETIT         P4            3          0.167   \n",
       "20         Hugo THOMAS         P2            1          0.100   \n",
       "21         Inès GARCIA         P7            1          0.100   \n",
       "22      Iris BLANCHARD         P3            1          0.100   \n",
       "23          Jade COLIN        P11            1          0.100   \n",
       "24     Jeanne BERTRAND         P5            1          0.100   \n",
       "25       Jules LAURENT         P7            1          0.100   \n",
       "26       Justine LOPES         P8            1          0.100   \n",
       "27        Lila GAUTIER        P12            1          0.100   \n",
       "28          Lina MOREL         P1            1          0.100   \n",
       "29        Louis DURAND         P3            1          0.100   \n",
       "30       Lucas BERNARD         P1            2          0.133   \n",
       "31          Lucie NOËL         P1            1          0.100   \n",
       "32         Léa RICHARD         P3            1          0.100   \n",
       "33         Manon LEROY         P3            1          0.100   \n",
       "34        Marion DUMAS         P8            2          0.133   \n",
       "35      Marius MATHIEU         P2            1          0.100   \n",
       "36       Maxime MULLER         P4            1          0.100   \n",
       "37          Maël ANDRÉ         P3            1          0.100   \n",
       "38      Maëlle BOUCHET        P12            1          0.100   \n",
       "39         Mila DUPONT         P6            1          0.100   \n",
       "40       Nathan MOREAU         P5            2          0.133   \n",
       "41        Nina BARBIER         P2            1          0.100   \n",
       "42        Nino OLIVIER         P6            1          0.100   \n",
       "43         Noa NAVARRO         P9            1          0.100   \n",
       "44            Noé ROUX         P5            1          0.100   \n",
       "45       Océane GUERIN        P13            1          0.100   \n",
       "46         Oscar BOYER        P10            1          0.100   \n",
       "47    Pauline GAUTHIER         P6            2          0.133   \n",
       "48     Raphaël MERCIER         P6            2          0.133   \n",
       "49     Romane GONZALEZ         P4            2          0.133   \n",
       "50      Sacha FRANÇOIS         P9            1          0.100   \n",
       "51       Salomé COUSIN         P6            1          0.100   \n",
       "52      Sarah LEFEBVRE         P7            1          0.100   \n",
       "53        Théo FONTANA        P12            1          0.100   \n",
       "54         Timéo PIRES         P6            2          0.133   \n",
       "55          Tom MICHEL         P7            1          0.100   \n",
       "56    Valentin CHARLES         P3            1          0.100   \n",
       "57      Victor LAMBERT         P8            2          0.133   \n",
       "58        Yanis RENAUD         P8            1          0.100   \n",
       "59         Zoé VINCENT         P5            1          0.100   \n",
       "60        Élise BONNET         P8            1          0.100   \n",
       "61           Émile REY         P7            1          0.100   \n",
       "\n",
       "              project_label initial_choice  \n",
       "0             Data Cleaning              1  \n",
       "1   Time Series Forecasting              1  \n",
       "2           Computer Vision              1  \n",
       "3             Data Cleaning              2  \n",
       "4             Web Analytics              1  \n",
       "5               NLP Chatbot              1  \n",
       "6               NLP Chatbot              1  \n",
       "7        Recommender System              1  \n",
       "8    Reinforcement Learning              1  \n",
       "9           Computer Vision              1  \n",
       "10             Graph Mining              1  \n",
       "11  Time Series Forecasting              2  \n",
       "12              NLP Chatbot              3  \n",
       "13                  Edge AI              1  \n",
       "14                  Edge AI              1  \n",
       "15            Data Cleaning              1  \n",
       "16       Recommender System              1  \n",
       "17             Graph Mining              1  \n",
       "18       Recommender System              1  \n",
       "19              NLP Chatbot              3  \n",
       "20       Recommender System              1  \n",
       "21   Reinforcement Learning              1  \n",
       "22     Image Classification              1  \n",
       "23             Graph Mining              1  \n",
       "24  Time Series Forecasting              1  \n",
       "25   Reinforcement Learning              1  \n",
       "26      Optimization Engine              1  \n",
       "27          AutoML Pipeline              1  \n",
       "28            Data Cleaning              1  \n",
       "29     Image Classification              1  \n",
       "30            Data Cleaning              2  \n",
       "31            Data Cleaning              1  \n",
       "32     Image Classification              1  \n",
       "33     Image Classification              1  \n",
       "34      Optimization Engine              2  \n",
       "35       Recommender System              1  \n",
       "36              NLP Chatbot              1  \n",
       "37     Image Classification              1  \n",
       "38          AutoML Pipeline              1  \n",
       "39        Anomaly Detection              1  \n",
       "40  Time Series Forecasting              2  \n",
       "41       Recommender System              1  \n",
       "42        Anomaly Detection              1  \n",
       "43            Web Analytics              1  \n",
       "44  Time Series Forecasting              1  \n",
       "45                  Edge AI              1  \n",
       "46          Computer Vision              1  \n",
       "47        Anomaly Detection              2  \n",
       "48        Anomaly Detection              2  \n",
       "49              NLP Chatbot              2  \n",
       "50            Web Analytics              1  \n",
       "51        Anomaly Detection              1  \n",
       "52   Reinforcement Learning              1  \n",
       "53          AutoML Pipeline              1  \n",
       "54        Anomaly Detection              2  \n",
       "55   Reinforcement Learning              1  \n",
       "56     Image Classification              1  \n",
       "57      Optimization Engine              2  \n",
       "58      Optimization Engine              1  \n",
       "59  Time Series Forecasting              1  \n",
       "60      Optimization Engine              1  \n",
       "61   Reinforcement Learning              1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Unweighted — Projects ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project_label</th>\n",
       "      <th>project_id</th>\n",
       "      <th>effectif</th>\n",
       "      <th>students</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Cleaning</td>\n",
       "      <td>P1</td>\n",
       "      <td>6</td>\n",
       "      <td>Aaron HUMBERT;Alice MARTIN;Eliott CHARPENTIER;Lina MOREL;Lucas BERNARD;Lucie NOËL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Recommender System</td>\n",
       "      <td>P2</td>\n",
       "      <td>6</td>\n",
       "      <td>Antoine PERROT;Emma DUBOIS;Ethan GIRARD;Hugo THOMAS;Marius MATHIEU;Nina BARBIER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Image Classification</td>\n",
       "      <td>P3</td>\n",
       "      <td>6</td>\n",
       "      <td>Iris BLANCHARD;Louis DURAND;Léa RICHARD;Manon LEROY;Maël ANDRÉ;Valentin CHARLES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NLP Chatbot</td>\n",
       "      <td>P4</td>\n",
       "      <td>6</td>\n",
       "      <td>Ambre FABRE;Anaïs LÉFÈVRE;Chloé ROBERT;Gabriel PETIT;Maxime MULLER;Romane GONZALEZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Time Series Forecasting</td>\n",
       "      <td>P5</td>\n",
       "      <td>6</td>\n",
       "      <td>Adam FOURNIER;Camille SIMON;Jeanne BERTRAND;Nathan MOREAU;Noé ROUX;Zoé VINCENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Anomaly Detection</td>\n",
       "      <td>P6</td>\n",
       "      <td>6</td>\n",
       "      <td>Mila DUPONT;Nino OLIVIER;Pauline GAUTHIER;Raphaël MERCIER;Salomé COUSIN;Timéo PIRES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Reinforcement Learning</td>\n",
       "      <td>P7</td>\n",
       "      <td>6</td>\n",
       "      <td>Arthur DAVID;Inès GARCIA;Jules LAURENT;Sarah LEFEBVRE;Tom MICHEL;Émile REY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Optimization Engine</td>\n",
       "      <td>P8</td>\n",
       "      <td>5</td>\n",
       "      <td>Justine LOPES;Marion DUMAS;Victor LAMBERT;Yanis RENAUD;Élise BONNET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Web Analytics</td>\n",
       "      <td>P9</td>\n",
       "      <td>3</td>\n",
       "      <td>Amandine RENARD;Noa NAVARRO;Sacha FRANÇOIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Computer Vision</td>\n",
       "      <td>P10</td>\n",
       "      <td>3</td>\n",
       "      <td>Agathe DUPUIS;Aya MARTINEZ;Oscar BOYER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Graph Mining</td>\n",
       "      <td>P11</td>\n",
       "      <td>3</td>\n",
       "      <td>Baptiste LEGRAND;Enzo PAUL;Jade COLIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AutoML Pipeline</td>\n",
       "      <td>P12</td>\n",
       "      <td>3</td>\n",
       "      <td>Lila GAUTIER;Maëlle BOUCHET;Théo FONTANA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Edge AI</td>\n",
       "      <td>P13</td>\n",
       "      <td>3</td>\n",
       "      <td>Clara CARON;Clément MARCHAND;Océane GUERIN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              project_label project_id  effectif  \\\n",
       "0             Data Cleaning         P1         6   \n",
       "1        Recommender System         P2         6   \n",
       "2      Image Classification         P3         6   \n",
       "3               NLP Chatbot         P4         6   \n",
       "4   Time Series Forecasting         P5         6   \n",
       "5         Anomaly Detection         P6         6   \n",
       "6    Reinforcement Learning         P7         6   \n",
       "7       Optimization Engine         P8         5   \n",
       "8             Web Analytics         P9         3   \n",
       "9           Computer Vision        P10         3   \n",
       "10             Graph Mining        P11         3   \n",
       "11          AutoML Pipeline        P12         3   \n",
       "12                  Edge AI        P13         3   \n",
       "\n",
       "                                                                               students  \n",
       "0     Aaron HUMBERT;Alice MARTIN;Eliott CHARPENTIER;Lina MOREL;Lucas BERNARD;Lucie NOËL  \n",
       "1       Antoine PERROT;Emma DUBOIS;Ethan GIRARD;Hugo THOMAS;Marius MATHIEU;Nina BARBIER  \n",
       "2       Iris BLANCHARD;Louis DURAND;Léa RICHARD;Manon LEROY;Maël ANDRÉ;Valentin CHARLES  \n",
       "3    Ambre FABRE;Anaïs LÉFÈVRE;Chloé ROBERT;Gabriel PETIT;Maxime MULLER;Romane GONZALEZ  \n",
       "4        Adam FOURNIER;Camille SIMON;Jeanne BERTRAND;Nathan MOREAU;Noé ROUX;Zoé VINCENT  \n",
       "5   Mila DUPONT;Nino OLIVIER;Pauline GAUTHIER;Raphaël MERCIER;Salomé COUSIN;Timéo PIRES  \n",
       "6            Arthur DAVID;Inès GARCIA;Jules LAURENT;Sarah LEFEBVRE;Tom MICHEL;Émile REY  \n",
       "7                   Justine LOPES;Marion DUMAS;Victor LAMBERT;Yanis RENAUD;Élise BONNET  \n",
       "8                                            Amandine RENARD;Noa NAVARRO;Sacha FRANÇOIS  \n",
       "9                                                Agathe DUPUIS;Aya MARTINEZ;Oscar BOYER  \n",
       "10                                                Baptiste LEGRAND;Enzo PAUL;Jade COLIN  \n",
       "11                                             Lila GAUTIER;Maëlle BOUCHET;Théo FONTANA  \n",
       "12                                           Clara CARON;Clément MARCHAND;Océane GUERIN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Weighted — Students ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student</th>\n",
       "      <th>project_id</th>\n",
       "      <th>choice_rank</th>\n",
       "      <th>choice_weight</th>\n",
       "      <th>project_label</th>\n",
       "      <th>initial_choice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adam FOURNIER</td>\n",
       "      <td>P5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Time Series Forecasting</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Agathe DUPUIS</td>\n",
       "      <td>P10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Computer Vision</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alice MARTIN</td>\n",
       "      <td>P1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.133</td>\n",
       "      <td>Data Cleaning</td>\n",
       "      <td>2:0.133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amandine RENARD</td>\n",
       "      <td>P9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Web Analytics</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ambre FABRE</td>\n",
       "      <td>P4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>NLP Chatbot</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Anaïs LÉFÈVRE</td>\n",
       "      <td>P4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>NLP Chatbot</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Arthur DAVID</td>\n",
       "      <td>P7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Reinforcement Learning</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Aya MARTINEZ</td>\n",
       "      <td>P10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Computer Vision</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Baptiste LEGRAND</td>\n",
       "      <td>P11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Graph Mining</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Camille SIMON</td>\n",
       "      <td>P5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.133</td>\n",
       "      <td>Time Series Forecasting</td>\n",
       "      <td>2:0.133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Chloé ROBERT</td>\n",
       "      <td>P6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.233</td>\n",
       "      <td>Anomaly Detection</td>\n",
       "      <td>5:0.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Clara CARON</td>\n",
       "      <td>P13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Edge AI</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Clément MARCHAND</td>\n",
       "      <td>P13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Edge AI</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Emma DUBOIS</td>\n",
       "      <td>P2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Recommender System</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Enzo PAUL</td>\n",
       "      <td>P11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Graph Mining</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Ethan GIRARD</td>\n",
       "      <td>P6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.233</td>\n",
       "      <td>Anomaly Detection</td>\n",
       "      <td>5:0.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Gabriel PETIT</td>\n",
       "      <td>P6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.233</td>\n",
       "      <td>Anomaly Detection</td>\n",
       "      <td>5:0.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Hugo THOMAS</td>\n",
       "      <td>P4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.167</td>\n",
       "      <td>NLP Chatbot</td>\n",
       "      <td>3:0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Inès GARCIA</td>\n",
       "      <td>P7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Reinforcement Learning</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Jade COLIN</td>\n",
       "      <td>P11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Graph Mining</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Jeanne BERTRAND</td>\n",
       "      <td>P5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Time Series Forecasting</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Jules LAURENT</td>\n",
       "      <td>P7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Reinforcement Learning</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Justine LOPES</td>\n",
       "      <td>P8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Optimization Engine</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Lila GAUTIER</td>\n",
       "      <td>P12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>AutoML Pipeline</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Lina MOREL</td>\n",
       "      <td>P1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Data Cleaning</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Louis DURAND</td>\n",
       "      <td>P3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Image Classification</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Lucas BERNARD</td>\n",
       "      <td>P1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.133</td>\n",
       "      <td>Data Cleaning</td>\n",
       "      <td>2:0.133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Lucie NOËL</td>\n",
       "      <td>P1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Data Cleaning</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Léa RICHARD</td>\n",
       "      <td>P3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Image Classification</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Manon LEROY</td>\n",
       "      <td>P3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Image Classification</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Marion DUMAS</td>\n",
       "      <td>P7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Reinforcement Learning</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Maxime MULLER</td>\n",
       "      <td>P4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>NLP Chatbot</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Maël ANDRÉ</td>\n",
       "      <td>P4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.133</td>\n",
       "      <td>NLP Chatbot</td>\n",
       "      <td>2:0.133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Maëlle BOUCHET</td>\n",
       "      <td>P12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>AutoML Pipeline</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Mila DUPONT</td>\n",
       "      <td>P6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Anomaly Detection</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Nathan MOREAU</td>\n",
       "      <td>P5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.133</td>\n",
       "      <td>Time Series Forecasting</td>\n",
       "      <td>2:0.133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Nino OLIVIER</td>\n",
       "      <td>P6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Anomaly Detection</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Noa NAVARRO</td>\n",
       "      <td>P9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Web Analytics</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Noé ROUX</td>\n",
       "      <td>P5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Time Series Forecasting</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Océane GUERIN</td>\n",
       "      <td>P13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Edge AI</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Oscar BOYER</td>\n",
       "      <td>P10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Computer Vision</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Pauline GAUTHIER</td>\n",
       "      <td>P6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.133</td>\n",
       "      <td>Anomaly Detection</td>\n",
       "      <td>2:0.133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Raphaël MERCIER</td>\n",
       "      <td>P6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.133</td>\n",
       "      <td>Anomaly Detection</td>\n",
       "      <td>2:0.133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Sacha FRANÇOIS</td>\n",
       "      <td>P9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Web Analytics</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Salomé COUSIN</td>\n",
       "      <td>P6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Anomaly Detection</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Sarah LEFEBVRE</td>\n",
       "      <td>P7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Reinforcement Learning</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Théo FONTANA</td>\n",
       "      <td>P12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>AutoML Pipeline</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Timéo PIRES</td>\n",
       "      <td>P6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.133</td>\n",
       "      <td>Anomaly Detection</td>\n",
       "      <td>2:0.133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Tom MICHEL</td>\n",
       "      <td>P7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Reinforcement Learning</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Valentin CHARLES</td>\n",
       "      <td>P4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.133</td>\n",
       "      <td>NLP Chatbot</td>\n",
       "      <td>2:0.133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Victor LAMBERT</td>\n",
       "      <td>P8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.133</td>\n",
       "      <td>Optimization Engine</td>\n",
       "      <td>2:0.133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Yanis RENAUD</td>\n",
       "      <td>P8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Optimization Engine</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Zoé VINCENT</td>\n",
       "      <td>P5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Time Series Forecasting</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Élise BONNET</td>\n",
       "      <td>P8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>Optimization Engine</td>\n",
       "      <td>1:0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Émile REY</td>\n",
       "      <td>P8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.133</td>\n",
       "      <td>Optimization Engine</td>\n",
       "      <td>2:0.133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             student project_id  choice_rank  choice_weight  \\\n",
       "0      Adam FOURNIER         P5            1          0.100   \n",
       "1      Agathe DUPUIS        P10            1          0.100   \n",
       "2       Alice MARTIN         P1            2          0.133   \n",
       "3    Amandine RENARD         P9            1          0.100   \n",
       "4        Ambre FABRE         P4            1          0.100   \n",
       "5      Anaïs LÉFÈVRE         P4            1          0.100   \n",
       "6       Arthur DAVID         P7            1          0.100   \n",
       "7       Aya MARTINEZ        P10            1          0.100   \n",
       "8   Baptiste LEGRAND        P11            1          0.100   \n",
       "9      Camille SIMON         P5            2          0.133   \n",
       "10      Chloé ROBERT         P6            5          0.233   \n",
       "11       Clara CARON        P13            1          0.100   \n",
       "12  Clément MARCHAND        P13            1          0.100   \n",
       "13       Emma DUBOIS         P2            1          0.100   \n",
       "14         Enzo PAUL        P11            1          0.100   \n",
       "15      Ethan GIRARD         P6            5          0.233   \n",
       "16     Gabriel PETIT         P6            5          0.233   \n",
       "17       Hugo THOMAS         P4            3          0.167   \n",
       "18       Inès GARCIA         P7            1          0.100   \n",
       "19        Jade COLIN        P11            1          0.100   \n",
       "20   Jeanne BERTRAND         P5            1          0.100   \n",
       "21     Jules LAURENT         P7            1          0.100   \n",
       "22     Justine LOPES         P8            1          0.100   \n",
       "23      Lila GAUTIER        P12            1          0.100   \n",
       "24        Lina MOREL         P1            1          0.100   \n",
       "25      Louis DURAND         P3            1          0.100   \n",
       "26     Lucas BERNARD         P1            2          0.133   \n",
       "27        Lucie NOËL         P1            1          0.100   \n",
       "28       Léa RICHARD         P3            1          0.100   \n",
       "29       Manon LEROY         P3            1          0.100   \n",
       "30      Marion DUMAS         P7            1          0.100   \n",
       "31     Maxime MULLER         P4            1          0.100   \n",
       "32        Maël ANDRÉ         P4            2          0.133   \n",
       "33    Maëlle BOUCHET        P12            1          0.100   \n",
       "34       Mila DUPONT         P6            1          0.100   \n",
       "35     Nathan MOREAU         P5            2          0.133   \n",
       "36      Nino OLIVIER         P6            1          0.100   \n",
       "37       Noa NAVARRO         P9            1          0.100   \n",
       "38          Noé ROUX         P5            1          0.100   \n",
       "39     Océane GUERIN        P13            1          0.100   \n",
       "40       Oscar BOYER        P10            1          0.100   \n",
       "41  Pauline GAUTHIER         P6            2          0.133   \n",
       "42   Raphaël MERCIER         P6            2          0.133   \n",
       "43    Sacha FRANÇOIS         P9            1          0.100   \n",
       "44     Salomé COUSIN         P6            1          0.100   \n",
       "45    Sarah LEFEBVRE         P7            1          0.100   \n",
       "46      Théo FONTANA        P12            1          0.100   \n",
       "47       Timéo PIRES         P6            2          0.133   \n",
       "48        Tom MICHEL         P7            1          0.100   \n",
       "49  Valentin CHARLES         P4            2          0.133   \n",
       "50    Victor LAMBERT         P8            2          0.133   \n",
       "51      Yanis RENAUD         P8            1          0.100   \n",
       "52       Zoé VINCENT         P5            1          0.100   \n",
       "53      Élise BONNET         P8            1          0.100   \n",
       "54         Émile REY         P8            2          0.133   \n",
       "\n",
       "              project_label initial_choice  \n",
       "0   Time Series Forecasting        1:0.100  \n",
       "1           Computer Vision        1:0.100  \n",
       "2             Data Cleaning        2:0.133  \n",
       "3             Web Analytics        1:0.100  \n",
       "4               NLP Chatbot        1:0.100  \n",
       "5               NLP Chatbot        1:0.100  \n",
       "6    Reinforcement Learning        1:0.100  \n",
       "7           Computer Vision        1:0.100  \n",
       "8              Graph Mining        1:0.100  \n",
       "9   Time Series Forecasting        2:0.133  \n",
       "10        Anomaly Detection        5:0.233  \n",
       "11                  Edge AI        1:0.100  \n",
       "12                  Edge AI        1:0.100  \n",
       "13       Recommender System        1:0.100  \n",
       "14             Graph Mining        1:0.100  \n",
       "15        Anomaly Detection        5:0.233  \n",
       "16        Anomaly Detection        5:0.233  \n",
       "17              NLP Chatbot        3:0.167  \n",
       "18   Reinforcement Learning        1:0.100  \n",
       "19             Graph Mining        1:0.100  \n",
       "20  Time Series Forecasting        1:0.100  \n",
       "21   Reinforcement Learning        1:0.100  \n",
       "22      Optimization Engine        1:0.100  \n",
       "23          AutoML Pipeline        1:0.100  \n",
       "24            Data Cleaning        1:0.100  \n",
       "25     Image Classification        1:0.100  \n",
       "26            Data Cleaning        2:0.133  \n",
       "27            Data Cleaning        1:0.100  \n",
       "28     Image Classification        1:0.100  \n",
       "29     Image Classification        1:0.100  \n",
       "30   Reinforcement Learning        1:0.100  \n",
       "31              NLP Chatbot        1:0.100  \n",
       "32              NLP Chatbot        2:0.133  \n",
       "33          AutoML Pipeline        1:0.100  \n",
       "34        Anomaly Detection        1:0.100  \n",
       "35  Time Series Forecasting        2:0.133  \n",
       "36        Anomaly Detection        1:0.100  \n",
       "37            Web Analytics        1:0.100  \n",
       "38  Time Series Forecasting        1:0.100  \n",
       "39                  Edge AI        1:0.100  \n",
       "40          Computer Vision        1:0.100  \n",
       "41        Anomaly Detection        2:0.133  \n",
       "42        Anomaly Detection        2:0.133  \n",
       "43            Web Analytics        1:0.100  \n",
       "44        Anomaly Detection        1:0.100  \n",
       "45   Reinforcement Learning        1:0.100  \n",
       "46          AutoML Pipeline        1:0.100  \n",
       "47        Anomaly Detection        2:0.133  \n",
       "48   Reinforcement Learning        1:0.100  \n",
       "49              NLP Chatbot        2:0.133  \n",
       "50      Optimization Engine        2:0.133  \n",
       "51      Optimization Engine        1:0.100  \n",
       "52  Time Series Forecasting        1:0.100  \n",
       "53      Optimization Engine        1:0.100  \n",
       "54      Optimization Engine        2:0.133  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Weighted — Projects ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project_label</th>\n",
       "      <th>project_id</th>\n",
       "      <th>effectif</th>\n",
       "      <th>students</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Cleaning</td>\n",
       "      <td>P1</td>\n",
       "      <td>4</td>\n",
       "      <td>Alice MARTIN;Lina MOREL;Lucas BERNARD;Lucie NOËL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Recommender System</td>\n",
       "      <td>P2</td>\n",
       "      <td>1</td>\n",
       "      <td>Emma DUBOIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Image Classification</td>\n",
       "      <td>P3</td>\n",
       "      <td>3</td>\n",
       "      <td>Louis DURAND;Léa RICHARD;Manon LEROY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NLP Chatbot</td>\n",
       "      <td>P4</td>\n",
       "      <td>6</td>\n",
       "      <td>Ambre FABRE;Anaïs LÉFÈVRE;Hugo THOMAS;Maxime MULLER;Maël ANDRÉ;Valentin CHARLES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Time Series Forecasting</td>\n",
       "      <td>P5</td>\n",
       "      <td>6</td>\n",
       "      <td>Adam FOURNIER;Camille SIMON;Jeanne BERTRAND;Nathan MOREAU;Noé ROUX;Zoé VINCENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Anomaly Detection</td>\n",
       "      <td>P6</td>\n",
       "      <td>9</td>\n",
       "      <td>Chloé ROBERT;Ethan GIRARD;Gabriel PETIT;Mila DUPONT;Nino OLIVIER;Pauline GAUTHIER;Raphaël MERCIER;Salomé COUSIN;Timéo PIRES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Reinforcement Learning</td>\n",
       "      <td>P7</td>\n",
       "      <td>6</td>\n",
       "      <td>Arthur DAVID;Inès GARCIA;Jules LAURENT;Marion DUMAS;Sarah LEFEBVRE;Tom MICHEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Optimization Engine</td>\n",
       "      <td>P8</td>\n",
       "      <td>5</td>\n",
       "      <td>Justine LOPES;Victor LAMBERT;Yanis RENAUD;Élise BONNET;Émile REY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Web Analytics</td>\n",
       "      <td>P9</td>\n",
       "      <td>3</td>\n",
       "      <td>Amandine RENARD;Noa NAVARRO;Sacha FRANÇOIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Computer Vision</td>\n",
       "      <td>P10</td>\n",
       "      <td>3</td>\n",
       "      <td>Agathe DUPUIS;Aya MARTINEZ;Oscar BOYER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Graph Mining</td>\n",
       "      <td>P11</td>\n",
       "      <td>3</td>\n",
       "      <td>Baptiste LEGRAND;Enzo PAUL;Jade COLIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AutoML Pipeline</td>\n",
       "      <td>P12</td>\n",
       "      <td>3</td>\n",
       "      <td>Lila GAUTIER;Maëlle BOUCHET;Théo FONTANA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Edge AI</td>\n",
       "      <td>P13</td>\n",
       "      <td>3</td>\n",
       "      <td>Clara CARON;Clément MARCHAND;Océane GUERIN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              project_label project_id  effectif  \\\n",
       "0             Data Cleaning         P1         4   \n",
       "1        Recommender System         P2         1   \n",
       "2      Image Classification         P3         3   \n",
       "3               NLP Chatbot         P4         6   \n",
       "4   Time Series Forecasting         P5         6   \n",
       "5         Anomaly Detection         P6         9   \n",
       "6    Reinforcement Learning         P7         6   \n",
       "7       Optimization Engine         P8         5   \n",
       "8             Web Analytics         P9         3   \n",
       "9           Computer Vision        P10         3   \n",
       "10             Graph Mining        P11         3   \n",
       "11          AutoML Pipeline        P12         3   \n",
       "12                  Edge AI        P13         3   \n",
       "\n",
       "                                                                                                                       students  \n",
       "0                                                                              Alice MARTIN;Lina MOREL;Lucas BERNARD;Lucie NOËL  \n",
       "1                                                                                                                   Emma DUBOIS  \n",
       "2                                                                                          Louis DURAND;Léa RICHARD;Manon LEROY  \n",
       "3                                               Ambre FABRE;Anaïs LÉFÈVRE;Hugo THOMAS;Maxime MULLER;Maël ANDRÉ;Valentin CHARLES  \n",
       "4                                                Adam FOURNIER;Camille SIMON;Jeanne BERTRAND;Nathan MOREAU;Noé ROUX;Zoé VINCENT  \n",
       "5   Chloé ROBERT;Ethan GIRARD;Gabriel PETIT;Mila DUPONT;Nino OLIVIER;Pauline GAUTHIER;Raphaël MERCIER;Salomé COUSIN;Timéo PIRES  \n",
       "6                                                 Arthur DAVID;Inès GARCIA;Jules LAURENT;Marion DUMAS;Sarah LEFEBVRE;Tom MICHEL  \n",
       "7                                                              Justine LOPES;Victor LAMBERT;Yanis RENAUD;Élise BONNET;Émile REY  \n",
       "8                                                                                    Amandine RENARD;Noa NAVARRO;Sacha FRANÇOIS  \n",
       "9                                                                                        Agathe DUPUIS;Aya MARTINEZ;Oscar BOYER  \n",
       "10                                                                                        Baptiste LEGRAND;Enzo PAUL;Jade COLIN  \n",
       "11                                                                                     Lila GAUTIER;Maëlle BOUCHET;Théo FONTANA  \n",
       "12                                                                                   Clara CARON;Clément MARCHAND;Océane GUERIN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stats — Unweighted ===\n",
      " n  assigned  unassigned  median_rank   p_top1  p_top3\n",
      "62        62           0          1.0 0.806452     1.0\n",
      "\n",
      "=== Stats — Weighted ===\n",
      " n  assigned  unassigned  median_rank   p_top1   p_top3\n",
      "55        55           0          1.0 0.727273 0.945455\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Assignment pipeline using min-cost flow.\n",
    "\n",
    "Features:\n",
    "- Ordered and weighted preference modes.\n",
    "- Input validation and deterministic behavior.\n",
    "- Optional interpretation of weighted prefs as \"higher is better\".\n",
    "- Optional inclusion of an \"__NA__\" bucket when capacity is insufficient.\n",
    "- CSV outputs plus quick satisfaction statistics.\n",
    "- IPython display fallback to plain text when not available.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Tuple\n",
    "import warnings\n",
    "\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 0)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "\n",
    "def resolve_data_dir() -> Path:\n",
    "    \"\"\"\n",
    "    Resolve the data directory (../data if inside src/, otherwise ./data).\n",
    "    Create the folder if it does not exist.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        base_dir = Path(__file__).resolve().parent\n",
    "        root_dir = base_dir.parent\n",
    "    except NameError:\n",
    "        # When running in notebooks or REPL (__file__ undefined)\n",
    "        root_dir = Path.cwd()\n",
    "        if root_dir.name == \"src\" and (root_dir.parent / \"data\").exists():\n",
    "            root_dir = root_dir.parent\n",
    "    data_dir = (root_dir / \"data\").resolve()\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return data_dir\n",
    "\n",
    "\n",
    "def load_projects_df(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load projects.csv with columns:\n",
    "      - id (required, unique)\n",
    "      - label (optional -> default = id)\n",
    "      - capacity (optional -> default = 1; must be >= 0)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, dtype=str).fillna(\"\")\n",
    "    if \"id\" not in df.columns:\n",
    "        raise ValueError(\"projects.csv must contain column 'id'.\")\n",
    "    if \"label\" not in df.columns:\n",
    "        df[\"label\"] = df[\"id\"]\n",
    "    if \"capacity\" not in df.columns:\n",
    "        df[\"capacity\"] = \"1\"\n",
    "\n",
    "    df[\"id\"] = df[\"id\"].str.strip()\n",
    "    df[\"label\"] = df[\"label\"].str.strip()\n",
    "    df[\"capacity\"] = (\n",
    "        pd.to_numeric(df[\"capacity\"].replace(\"\", \"1\"), errors=\"coerce\")\n",
    "        .fillna(1)\n",
    "        .astype(int)\n",
    "    )\n",
    "\n",
    "    # Validations\n",
    "    if df[\"id\"].duplicated().any():\n",
    "        dups = df.loc[df[\"id\"].duplicated(), \"id\"].tolist()\n",
    "        raise ValueError(f\"projects.csv: 'id' must be unique. Duplicates: {dups}\")\n",
    "    if (df[\"capacity\"] < 0).any():\n",
    "        negs = df.loc[df[\"capacity\"] < 0, \"id\"].tolist()\n",
    "        raise ValueError(\n",
    "            f\"projects.csv: 'capacity' must be >= 0. Offending projects: {negs}\"\n",
    "        )\n",
    "\n",
    "    return df[[\"id\", \"label\", \"capacity\"]]\n",
    "\n",
    "\n",
    "def _split_semicolon(s: str) -> List[str]:\n",
    "    return [x.strip() for x in (s or \"\").split(\";\") if x.strip()]\n",
    "\n",
    "\n",
    "def _parse_weighted_prefs(raw: str, valid: set[str]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Parse weighted preferences in format 'p1:1.5;p2:3' -> {'p1': 1.5, 'p2': 3.0}\n",
    "    Ignores unknown projects and non-numeric weights.\n",
    "    \"\"\"\n",
    "    result: Dict[str, float] = {}\n",
    "    for tok in _split_semicolon(raw):\n",
    "        if \":\" not in tok:\n",
    "            warnings.warn(\n",
    "                f\"Malformed weighted preference (missing ':') ignored: '{tok}'\",\n",
    "                RuntimeWarning,\n",
    "            )\n",
    "            continue\n",
    "        pid, w = tok.split(\":\", 1)\n",
    "        pid = pid.strip()\n",
    "        if pid not in valid:\n",
    "            warnings.warn(\n",
    "                f\"Unknown project in weighted prefs ignored: '{pid}'\",\n",
    "                RuntimeWarning,\n",
    "            )\n",
    "            continue\n",
    "        try:\n",
    "            result[pid] = float(w.strip())\n",
    "        except ValueError:\n",
    "            warnings.warn(\n",
    "                f\"Non-numeric weight ignored for '{pid}': '{w}'\", RuntimeWarning\n",
    "            )\n",
    "            continue\n",
    "    return result\n",
    "\n",
    "\n",
    "def load_choices_df(path: Path, valid_projects: Iterable[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load student-choices.csv with columns:\n",
    "      - student (required)\n",
    "      - prefs (required), e.g. \"p1;p2;p3\" or \"p1:0;p2:1.5\"\n",
    "      - weight (optional -> default 1; coerced to >= 1)\n",
    "      - names  (optional); if provided, must contain exactly `weight` names\n",
    "        separated by ';'. Otherwise auto-generated from 'student'.\n",
    "    Automatically detects 'ordered' vs 'weighted'.\n",
    "    Returns columns: key, weight, mode, prefs_ordered, prefs_weighted, names\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, dtype=str).fillna(\"\")\n",
    "    required = {\"student\", \"prefs\"}\n",
    "    if not required.issubset(df.columns):\n",
    "        raise ValueError(\n",
    "            \"student-choices.csv must contain at least 'student' and 'prefs'.\"\n",
    "        )\n",
    "\n",
    "    df[\"student\"] = df[\"student\"].str.strip()\n",
    "    df = df[df[\"student\"] != \"\"].copy()\n",
    "\n",
    "    if \"weight\" not in df.columns:\n",
    "        df[\"weight\"] = \"1\"\n",
    "    df[\"weight\"] = pd.to_numeric(df[\"weight\"], errors=\"coerce\").fillna(1).astype(int)\n",
    "    df.loc[df[\"weight\"] < 1, \"weight\"] = 1\n",
    "\n",
    "    valid = set(valid_projects)\n",
    "\n",
    "    modes: List[str] = []\n",
    "    prefs_ordered_col: List[List[str]] = []\n",
    "    prefs_weighted_col: List[Dict[str, float]] = []\n",
    "    names_col: List[List[str]] = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        key = row[\"student\"]\n",
    "        w = int(row[\"weight\"])\n",
    "        raw_prefs = str(row[\"prefs\"])\n",
    "        tokens = _split_semicolon(raw_prefs)\n",
    "        is_weighted = any(\":\" in tok for tok in tokens)\n",
    "\n",
    "        if is_weighted:\n",
    "            mode = \"weighted\"\n",
    "            weights_map = _parse_weighted_prefs(raw_prefs, valid)\n",
    "            if not weights_map:\n",
    "                # No valid 'p:w' tokens -> fallback to 'ordered' from plain ids\n",
    "                modes.append(\"ordered\")\n",
    "                ordered = [p for p in tokens if \":\" not in p and p in valid]\n",
    "                if not ordered:\n",
    "                    warnings.warn(\n",
    "                        f\"No valid preference found for '{key}'.\",\n",
    "                        RuntimeWarning,\n",
    "                    )\n",
    "                prefs_ordered_col.append(ordered)\n",
    "                prefs_weighted_col.append({})\n",
    "            else:\n",
    "                modes.append(mode)\n",
    "                # Deterministic ordering by (weight, pid)\n",
    "                ordered_pairs = sorted(weights_map.items(), key=lambda kv: (kv[1], kv[0]))\n",
    "                prefs_ordered_col.append([pid for pid, _ in ordered_pairs])\n",
    "                prefs_weighted_col.append(weights_map)\n",
    "        else:\n",
    "            mode = \"ordered\"\n",
    "            modes.append(mode)\n",
    "            ordered = [p for p in tokens if p in valid]\n",
    "            if not ordered:\n",
    "                warnings.warn(\n",
    "                    f\"No valid preference found for '{key}'.\",\n",
    "                    RuntimeWarning,\n",
    "                )\n",
    "            prefs_ordered_col.append(ordered)\n",
    "            prefs_weighted_col.append({})\n",
    "\n",
    "        raw_names = _split_semicolon(row.get(\"names\", \"\"))\n",
    "        if raw_names and len(raw_names) != w:\n",
    "            warnings.warn(\n",
    "                f\"'names' provided for '{key}' but length != weight \"\n",
    "                f\"({len(raw_names)} != {w}). Ignoring provided names.\",\n",
    "                RuntimeWarning,\n",
    "            )\n",
    "            raw_names = []\n",
    "        if not raw_names:\n",
    "            raw_names = [f\"{key}#{i + 1}\" if w > 1 else key for i in range(w)]\n",
    "        names_col.append(raw_names)\n",
    "\n",
    "    df = df.rename(columns={\"student\": \"key\"})\n",
    "    df[\"mode\"] = modes\n",
    "    df[\"prefs_ordered\"] = prefs_ordered_col\n",
    "    df[\"prefs_weighted\"] = prefs_weighted_col\n",
    "    df[\"names\"] = names_col\n",
    "    return df[\n",
    "        [\"key\", \"weight\", \"mode\", \"prefs_ordered\", \"prefs_weighted\", \"names\"]\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def _common_graph_skeleton(\n",
    "    projects_df: pd.DataFrame,\n",
    "    total_students: int,\n",
    "    unassigned_label: str,\n",
    ") -> Tuple[nx.DiGraph, str, str, Dict[str, int], List[str], int]:\n",
    "    \"\"\"\n",
    "    Build the sink side of the flow network (projects -> t), and add source/sink\n",
    "    nodes with correct global demand.\n",
    "    \"\"\"\n",
    "    projects = projects_df[\"id\"].tolist()\n",
    "    capacities = dict(zip(projects_df[\"id\"], projects_df[\"capacity\"]))\n",
    "    cap = {p: int(capacities.get(p, 1)) for p in projects}\n",
    "    total_cap = sum(cap.values())\n",
    "    proj_ids = projects[:]\n",
    "    if total_cap < total_students:\n",
    "        proj_ids.append(unassigned_label)\n",
    "        cap[unassigned_label] = total_students - total_cap\n",
    "\n",
    "    # Deterministic ordering for stability\n",
    "    proj_ids = list(proj_ids)\n",
    "\n",
    "    g = nx.DiGraph()\n",
    "    s, t = \"_s\", \"_t\"\n",
    "    flow_target = min(total_students, sum(cap.values()))\n",
    "    g.add_node(s, demand=-flow_target)\n",
    "    g.add_node(t, demand=flow_target)\n",
    "    for p in proj_ids:\n",
    "        g.add_node(p, demand=0)\n",
    "        g.add_edge(p, t, capacity=cap[p], weight=0)\n",
    "    return g, s, t, cap, proj_ids, flow_target\n",
    "\n",
    "\n",
    "def build_graph_unweighted(\n",
    "    entries_df: pd.DataFrame,\n",
    "    projects_df: pd.DataFrame,\n",
    "    rank_cost: Optional[List[float]],\n",
    "    penalty: Optional[float],\n",
    "    unassigned_label: str = \"__NA__\",\n",
    ") -> Tuple[nx.DiGraph, Dict]:\n",
    "    \"\"\"\n",
    "    Build graph for 'ordered' mode.\n",
    "    - rank_cost: list of costs per rank (default: 0 for best, 1 for next, ...).\n",
    "      If the list is too short, extrapolate linearly with slope=1.\n",
    "    - penalty: cost for any non-listed project (default = max(rank_cost) + 5).\n",
    "    \"\"\"\n",
    "    n_people = int(entries_df[\"weight\"].sum())\n",
    "    if rank_cost is None:\n",
    "        max_len = int(entries_df[\"prefs_ordered\"].map(len).max()) if len(entries_df) else 1\n",
    "        rank_cost = list(range(max(1, max_len)))  # [0, 1, 2, ..., L-1]\n",
    "    if penalty is None:\n",
    "        penalty = (max(rank_cost) if rank_cost else 5) + 5\n",
    "\n",
    "    g, s, t, cap, proj_ids, flow_target = _common_graph_skeleton(\n",
    "        projects_df, n_people, unassigned_label\n",
    "    )\n",
    "\n",
    "    for i, row in entries_df.reset_index(drop=True).iterrows():\n",
    "        u = f\"e{i}\"\n",
    "        g.add_node(u, demand=0)\n",
    "        g.add_edge(s, u, capacity=int(row[\"weight\"]), weight=0)\n",
    "\n",
    "    for i, row in entries_df.reset_index(drop=True).iterrows():\n",
    "        u = f\"e{i}\"\n",
    "        prefs: List[str] = row[\"prefs_ordered\"]\n",
    "        rank_map = {p: r for r, p in enumerate(prefs)}\n",
    "        for p in proj_ids:\n",
    "            if p in rank_map:\n",
    "                r = rank_map[p]\n",
    "                if r < len(rank_cost):\n",
    "                    cost = rank_cost[r]\n",
    "                else:\n",
    "                    # Linear extrapolation past provided rank_cost\n",
    "                    cost = rank_cost[-1] + (r - (len(rank_cost) - 1))\n",
    "            else:\n",
    "                cost = penalty\n",
    "            g.add_edge(u, p, capacity=int(row[\"weight\"]), weight=float(cost))\n",
    "\n",
    "    meta = {\n",
    "        \"s\": s,\n",
    "        \"t\": t,\n",
    "        \"entries\": entries_df,\n",
    "        \"cap\": cap,\n",
    "        \"flow_target\": flow_target,\n",
    "        \"unassigned\": unassigned_label,\n",
    "    }\n",
    "    return g, meta\n",
    "\n",
    "\n",
    "def build_graph_weighted(\n",
    "    entries_df: pd.DataFrame,\n",
    "    projects_df: pd.DataFrame,\n",
    "    penalty: Optional[float],\n",
    "    unassigned_label: str = \"__NA__\",\n",
    "    higher_is_better: bool = False,\n",
    ") -> Tuple[nx.DiGraph, Dict]:\n",
    "    \"\"\"\n",
    "    Build graph for 'weighted' mode.\n",
    "\n",
    "    Default meaning: 'weight' = cost (lower is better).\n",
    "    If higher_is_better=True, transform weights into costs using (max - w).\n",
    "\n",
    "    - penalty: cost for any non-listed project (default 10.0).\n",
    "    \"\"\"\n",
    "    n_people = int(entries_df[\"weight\"].sum())\n",
    "    if penalty is None:\n",
    "        penalty = 10.0\n",
    "\n",
    "    g, s, t, cap, proj_ids, flow_target = _common_graph_skeleton(\n",
    "        projects_df, n_people, unassigned_label\n",
    "    )\n",
    "\n",
    "    for i, row in entries_df.reset_index(drop=True).iterrows():\n",
    "        u = f\"e{i}\"\n",
    "        g.add_node(u, demand=0)\n",
    "        g.add_edge(s, u, capacity=int(row[\"weight\"]), weight=0)\n",
    "\n",
    "    for i, row in entries_df.reset_index(drop=True).iterrows():\n",
    "        u = f\"e{i}\"\n",
    "        wmap: Dict[str, float] = dict(row[\"prefs_weighted\"])\n",
    "        if higher_is_better and wmap:\n",
    "            mx = max(wmap.values())\n",
    "            # Convert to positive costs: cost = (mx - score)\n",
    "            wmap = {p: (mx - v) for p, v in wmap.items()}\n",
    "        for p in proj_ids:\n",
    "            cost = float(wmap[p]) if p in wmap else float(penalty)\n",
    "            g.add_edge(u, p, capacity=int(row[\"weight\"]), weight=cost)\n",
    "\n",
    "    meta = {\n",
    "        \"s\": s,\n",
    "        \"t\": t,\n",
    "        \"entries\": entries_df,\n",
    "        \"cap\": cap,\n",
    "        \"flow_target\": flow_target,\n",
    "        \"unassigned\": unassigned_label,\n",
    "    }\n",
    "    return g, meta\n",
    "\n",
    "\n",
    "def solve_min_cost(g: nx.DiGraph) -> Tuple[Dict, float]:\n",
    "    \"\"\"\n",
    "    Solve min-cost flow. Returns (flow_dict, total_cost).\n",
    "    \"\"\"\n",
    "    flow = nx.min_cost_flow(g, demand=\"demand\", capacity=\"capacity\", weight=\"weight\")\n",
    "    cost = nx.cost_of_flow(g, flow, weight=\"weight\")\n",
    "    return flow, float(cost)\n",
    "\n",
    "\n",
    "def expand_to_individual_rows(flow: Dict, meta: Dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Expand the flow into per-person rows.\n",
    "    Columns: student, project_id, choice_rank, choice_weight\n",
    "    \"\"\"\n",
    "    entries_df: pd.DataFrame = meta[\"entries\"]\n",
    "    unassigned = meta[\"unassigned\"]\n",
    "    rows: List[Tuple[str, Optional[str], Optional[int], Optional[float]]] = []\n",
    "\n",
    "    for i, row in entries_df.reset_index(drop=True).iterrows():\n",
    "        u = f\"e{i}\"\n",
    "        alloc = [(p, f) for p, f in flow[u].items() if f > 0]\n",
    "        names = list(row[\"names\"])\n",
    "        key = row[\"key\"]\n",
    "        ordered = row.get(\"prefs_ordered\", []) or []\n",
    "        rank_map = {p: (r + 1) for r, p in enumerate(ordered)}\n",
    "        wmap: Dict[str, float] = row.get(\"prefs_weighted\", {}) or {}\n",
    "\n",
    "        for p, k in alloc:\n",
    "            for _ in range(int(k)):\n",
    "                nm = names.pop(0) if names else f\"{key}#?\"\n",
    "                pid = None if p == unassigned else p\n",
    "                choice_rank = rank_map.get(p) if pid is not None else None\n",
    "                choice_weight = wmap.get(p) if pid is not None else None\n",
    "                rows.append((nm, pid, choice_rank, choice_weight))\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        rows, columns=[\"student\", \"project_id\", \"choice_rank\", \"choice_weight\"]\n",
    "    )\n",
    "\n",
    "\n",
    "def build_student_df(\n",
    "    assign_df: pd.DataFrame, projects_df: pd.DataFrame, mode: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Enrich assignment with project labels and an 'initial_choice' string.\n",
    "    \"\"\"\n",
    "    labels = dict(zip(projects_df[\"id\"], projects_df[\"label\"]))\n",
    "    df = assign_df.copy()\n",
    "    df[\"project_label\"] = df[\"project_id\"].map(labels).fillna(\n",
    "        df[\"project_id\"].fillna(\"\")\n",
    "    )\n",
    "\n",
    "    def _fmt(row: pd.Series) -> str:\n",
    "        r = row.get(\"choice_rank\")\n",
    "        w = row.get(\"choice_weight\")\n",
    "        if mode == \"weighted\":\n",
    "            if pd.notna(r) and pd.notna(w):\n",
    "                return f\"{int(r)}:{w:.3f}\"\n",
    "            if pd.notna(r):\n",
    "                return f\"{int(r)}:\"\n",
    "            if pd.notna(w):\n",
    "                return f\":{w:.3f}\"\n",
    "            return \"\"\n",
    "        return str(int(r)) if pd.notna(r) else \"\"\n",
    "\n",
    "    df[\"initial_choice\"] = df.apply(_fmt, axis=1)\n",
    "    return df.sort_values(\"student\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "def build_project_df(\n",
    "    assign_df: pd.DataFrame,\n",
    "    projects_df: pd.DataFrame,\n",
    "    include_unassigned: str | bool = \"auto\",\n",
    "    unassigned_label: str = \"__NA__\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate by project.\n",
    "\n",
    "    include_unassigned:\n",
    "      - True  : always add the 'unassigned_label' row\n",
    "      - False : never add it\n",
    "      - 'auto': add only if unassigned exist\n",
    "    \"\"\"\n",
    "    order = projects_df[\"id\"].tolist()\n",
    "    labels = dict(zip(projects_df[\"id\"], projects_df[\"label\"]))\n",
    "    grouped = assign_df.groupby(\"project_id\")[\"student\"].apply(list).to_dict()\n",
    "    rows = []\n",
    "    for pid in order:\n",
    "        students = sorted(grouped.get(pid, []))\n",
    "        rows.append([labels.get(pid, pid), pid, len(students), \";\".join(students)])\n",
    "\n",
    "    add_unassigned = (include_unassigned is True) or (\n",
    "        include_unassigned == \"auto\" and unassigned_label in grouped\n",
    "    )\n",
    "    if add_unassigned:\n",
    "        students = sorted(grouped.get(unassigned_label, []))\n",
    "        rows.append(\n",
    "            [unassigned_label, unassigned_label, len(students), \";\".join(students)]\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        rows, columns=[\"project_label\", \"project_id\", \"effectif\", \"students\"]\n",
    "    )\n",
    "\n",
    "\n",
    "def write_students_csv(df_students: pd.DataFrame, path: Path) -> None:\n",
    "    df = df_students[[\"student\", \"project_id\", \"project_label\", \"initial_choice\"]].copy()\n",
    "    df.to_csv(path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "def write_projects_csv(df_projects: pd.DataFrame, path: Path) -> None:\n",
    "    df_projects.to_csv(path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "def satisfaction_stats(df_students: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return quick satisfaction metrics.\n",
    "    \"\"\"\n",
    "    s = df_students.copy()\n",
    "    # Extract rank (before ':') if present\n",
    "    s[\"rank\"] = pd.to_numeric(\n",
    "        s[\"initial_choice\"].str.split(\":\").str[0],\n",
    "        errors=\"coerce\",\n",
    "    )\n",
    "    out = {\n",
    "        \"n\": len(s),\n",
    "        \"assigned\": int(s[\"project_id\"].notna().sum()),\n",
    "        \"unassigned\": int(s[\"project_id\"].isna().sum()),\n",
    "        \"median_rank\": float(s[\"rank\"].median()) if len(s) else float(\"nan\"),\n",
    "        \"p_top1\": float((s[\"rank\"] == 1).mean()) if len(s) else float(\"nan\"),\n",
    "        \"p_top3\": float((s[\"rank\"] <= 3).mean()) if len(s) else float(\"nan\"),\n",
    "    }\n",
    "    return pd.DataFrame([out])\n",
    "\n",
    "\n",
    "def run_pipeline_unweighted(\n",
    "    data_dir: Path,\n",
    "    rank_cost: Optional[List[float]] = None,\n",
    "    penalty: Optional[float] = None,\n",
    "    unassigned_label: str = \"__NA__\",\n",
    "    write_outputs: bool = True,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, float, Dict[str, Path]]:\n",
    "    \"\"\"\n",
    "    Run the ordered-preferences pipeline end-to-end.\n",
    "    \"\"\"\n",
    "    paths = {\n",
    "        \"projects\": data_dir / \"projects.csv\",\n",
    "        \"choices\": data_dir / \"student-choices.csv\",\n",
    "        \"student_out\": data_dir / \"assignment_student_unweighted.csv\",\n",
    "        \"project_out\": data_dir / \"assignment_project_unweighted.csv\",\n",
    "    }\n",
    "    projects_df = load_projects_df(paths[\"projects\"])\n",
    "    choices_df = load_choices_df(paths[\"choices\"], projects_df[\"id\"].tolist())\n",
    "    g, meta = build_graph_unweighted(\n",
    "        choices_df, projects_df, rank_cost, penalty, unassigned_label\n",
    "    )\n",
    "    flow, cost = solve_min_cost(g)\n",
    "    assign_df = expand_to_individual_rows(flow, meta)\n",
    "    df_students = build_student_df(assign_df, projects_df, mode=\"unweighted\")\n",
    "    df_projects = build_project_df(\n",
    "        assign_df, projects_df, include_unassigned=\"auto\", unassigned_label=unassigned_label\n",
    "    )\n",
    "    if write_outputs:\n",
    "        write_students_csv(df_students, paths[\"student_out\"])\n",
    "        write_projects_csv(df_projects, paths[\"project_out\"])\n",
    "    return df_students, df_projects, cost, paths\n",
    "\n",
    "\n",
    "def run_pipeline_weighted(\n",
    "    data_dir: Path,\n",
    "    penalty: Optional[float] = None,\n",
    "    unassigned_label: str = \"__NA__\",\n",
    "    write_outputs: bool = True,\n",
    "    higher_is_better: bool = False,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, float, Dict[str, Path]]:\n",
    "    \"\"\"\n",
    "    Run the weighted-preferences pipeline end-to-end.\n",
    "    \"\"\"\n",
    "    paths = {\n",
    "        \"projects\": data_dir / \"projects.csv\",\n",
    "        \"choices\": data_dir / \"student-choices.csv\",\n",
    "        \"student_out\": data_dir / \"assignment_student_weighted.csv\",\n",
    "        \"project_out\": data_dir / \"assignment_project_weighted.csv\",\n",
    "    }\n",
    "    projects_df = load_projects_df(paths[\"projects\"])\n",
    "    choices_df = load_choices_df(paths[\"choices\"], projects_df[\"id\"].tolist())\n",
    "    g, meta = build_graph_weighted(\n",
    "        choices_df,\n",
    "        projects_df,\n",
    "        penalty,\n",
    "        unassigned_label,\n",
    "        higher_is_better=higher_is_better,\n",
    "    )\n",
    "    flow, cost = solve_min_cost(g)\n",
    "    assign_df = expand_to_individual_rows(flow, meta)\n",
    "    df_students = build_student_df(assign_df, projects_df, mode=\"weighted\")\n",
    "    df_projects = build_project_df(\n",
    "        assign_df, projects_df, include_unassigned=\"auto\", unassigned_label=unassigned_label\n",
    "    )\n",
    "    if write_outputs:\n",
    "        write_students_csv(df_students, paths[\"student_out\"])\n",
    "        write_projects_csv(df_projects, paths[\"project_out\"])\n",
    "    return df_students, df_projects, cost, paths\n",
    "\n",
    "\n",
    "def run_both(\n",
    "    data_dir: Optional[Path] = None,\n",
    "    rank_cost: Optional[List[float]] = None,\n",
    "    unweighted_penalty: Optional[float] = None,\n",
    "    weighted_penalty: Optional[float] = None,\n",
    "    unassigned_label: str = \"__NA__\",\n",
    "    write_outputs: bool = True,\n",
    "    higher_is_better: bool = False,\n",
    ") -> Dict[str, object]:\n",
    "    \"\"\"\n",
    "    Run both pipelines and return all outputs.\n",
    "    \"\"\"\n",
    "    data_dir = data_dir or resolve_data_dir()\n",
    "    stu_unw, prj_unw, cost_unw, paths_unw = run_pipeline_unweighted(\n",
    "        data_dir=data_dir,\n",
    "        rank_cost=rank_cost,\n",
    "        penalty=unweighted_penalty,\n",
    "        unassigned_label=unassigned_label,\n",
    "        write_outputs=write_outputs,\n",
    "    )\n",
    "    stu_w, prj_w, cost_w, paths_w = run_pipeline_weighted(\n",
    "        data_dir=data_dir,\n",
    "        penalty=weighted_penalty,\n",
    "        unassigned_label=unassigned_label,\n",
    "        write_outputs=write_outputs,\n",
    "        higher_is_better=higher_is_better,\n",
    "    )\n",
    "    return {\n",
    "        \"students_unweighted\": stu_unw,\n",
    "        \"projects_unweighted\": prj_unw,\n",
    "        \"cost_unweighted\": cost_unw,\n",
    "        \"paths_unweighted\": paths_unw,\n",
    "        \"students_weighted\": stu_w,\n",
    "        \"projects_weighted\": prj_w,\n",
    "        \"cost_weighted\": cost_w,\n",
    "        \"paths_weighted\": paths_w,\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    DATA_DIR = resolve_data_dir()\n",
    "    results = run_both(\n",
    "        data_dir=DATA_DIR,\n",
    "        rank_cost=None,\n",
    "        unweighted_penalty=None,\n",
    "        weighted_penalty=10.0,\n",
    "        unassigned_label=\"__NA__\",\n",
    "        write_outputs=True,\n",
    "        higher_is_better=False,  # set True if weights mean \"preference\" (higher = better)\n",
    "    )\n",
    "    print(\"Cost (unweighted):\", results[\"cost_unweighted\"])\n",
    "    print(\"Cost (weighted):  \", results[\"cost_weighted\"])\n",
    "    print(\n",
    "        \"Written:\",\n",
    "        results[\"paths_unweighted\"][\"student_out\"].name,\n",
    "        \",\",\n",
    "        results[\"paths_unweighted\"][\"project_out\"].name,\n",
    "        \",\",\n",
    "        results[\"paths_weighted\"][\"student_out\"].name,\n",
    "        \",\",\n",
    "        results[\"paths_weighted\"][\"project_out\"].name,\n",
    "    )\n",
    "\n",
    "    # IPython is optional; fallback to plain text if not available\n",
    "    try:\n",
    "        from IPython.display import display  # type: ignore\n",
    "    except Exception:\n",
    "        display = None  # type: ignore\n",
    "\n",
    "    def _show(df: pd.DataFrame, title: str) -> None:\n",
    "        print(f\"\\n=== {title} ===\")\n",
    "        if display:\n",
    "            display(df)  # type: ignore\n",
    "        else:\n",
    "            print(df.to_string(index=False))\n",
    "\n",
    "    _show(results[\"students_unweighted\"], \"Unweighted — Students\")\n",
    "    _show(results[\"projects_unweighted\"], \"Unweighted — Projects\")\n",
    "    _show(results[\"students_weighted\"], \"Weighted — Students\")\n",
    "    _show(results[\"projects_weighted\"], \"Weighted — Projects\")\n",
    "\n",
    "    # Quick satisfaction stats\n",
    "    try:\n",
    "        print(\"\\n=== Stats — Unweighted ===\")\n",
    "        print(satisfaction_stats(results[\"students_unweighted\"]).to_string(index=False))\n",
    "        print(\"\\n=== Stats — Weighted ===\")\n",
    "        print(satisfaction_stats(results[\"students_weighted\"]).to_string(index=False))\n",
    "    except Exception as exc:\n",
    "        warnings.warn(f\"Could not compute stats: {exc}\", RuntimeWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1981ef43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exports (unweighted):\n",
      "  graphml: exports\\unweighted_model.graphml\n",
      "  gexf: exports\\unweighted_model.gexf\n",
      "  gpickle: exports\\unweighted_model.gpickle\n",
      "  json: exports\\unweighted_model.json\n",
      "  csv: exports\\unweighted_model_flow_edges.csv\n",
      "  viz: exports\\unweighted_assignment.png\n",
      "\n",
      "Exports (weighted):\n",
      "  graphml: exports\\weighted_model.graphml\n",
      "  gexf: exports\\weighted_model.gexf\n",
      "  gpickle: exports\\weighted_model.gpickle\n",
      "  json: exports\\weighted_model.json\n",
      "  csv: exports\\weighted_model_flow_edges.csv\n",
      "  viz: exports\\weighted_assignment.png\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Export and visualization utilities for assignment graphs.\n",
    "\n",
    "Includes:\n",
    "- Graph export to GraphML, GEXF, GPickle (compatible), JSON (node-link),\n",
    "  and a CSV of edges with positive flow.\n",
    "- A simple bipartite visualization of the assignment (entries -> projects).\n",
    "\n",
    "All comments, messages, and output labels are in English.\n",
    "PEP 8 compliant.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 8) Export and visualization\n",
    "# ================================\n",
    "\n",
    "\n",
    "def _ensure_dir(path: Path) -> None:\n",
    "    \"\"\"Ensure a directory exists.\"\"\"\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def export_graph_models(\n",
    "    graph: nx.DiGraph,\n",
    "    meta: Dict,  # kept for API symmetry\n",
    "    flow: Dict,\n",
    "    export_dir: Path,\n",
    "    prefix: str = \"model\",\n",
    ") -> Dict[str, Path]:\n",
    "    \"\"\"\n",
    "    Export the given network with flow attributes in multiple formats:\n",
    "      - GraphML (.graphml)\n",
    "      - GEXF (.gexf)\n",
    "      - GPickle (.gpickle) : robust across nx versions (fallback to pickle)\n",
    "      - JSON node-link (.json)\n",
    "      - CSV of edges with positive flow (_flow_edges.csv)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    graph : nx.DiGraph\n",
    "        The full flow network.\n",
    "    meta : Dict\n",
    "        Metadata dict returned by build_graph_* (unused here).\n",
    "    flow : Dict\n",
    "        Flow dictionary as returned by solve_min_cost.\n",
    "    export_dir : Path\n",
    "        Destination directory.\n",
    "    prefix : str\n",
    "        Filename prefix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Path]\n",
    "        Mapping of format name -> written file path.\n",
    "    \"\"\"\n",
    "    _ensure_dir(export_dir)\n",
    "\n",
    "    graph_copy = graph.copy()\n",
    "    for u in flow:\n",
    "        for v, fval in flow[u].items():\n",
    "            if graph_copy.has_edge(u, v):\n",
    "                graph_copy[u][v][\"flow\"] = int(fval)\n",
    "\n",
    "    paths = {\n",
    "        \"graphml\": export_dir / f\"{prefix}.graphml\",\n",
    "        \"gexf\": export_dir / f\"{prefix}.gexf\",\n",
    "        \"gpickle\": export_dir / f\"{prefix}.gpickle\",\n",
    "        \"json\": export_dir / f\"{prefix}.json\",\n",
    "        \"csv\": export_dir / f\"{prefix}_flow_edges.csv\",\n",
    "    }\n",
    "\n",
    "    # GraphML / GEXF\n",
    "    nx.write_graphml(graph_copy, paths[\"graphml\"])\n",
    "    nx.write_gexf(graph_copy, paths[\"gexf\"])\n",
    "\n",
    "    # --- GPickle with compatibility across nx versions ---\n",
    "    try:\n",
    "        # NetworkX ≥ 3.x: write_gpickle may not be at top-level\n",
    "        from networkx.readwrite.gpickle import (  # type: ignore\n",
    "            write_gpickle as _write_gpickle,\n",
    "        )\n",
    "\n",
    "        _write_gpickle(graph_copy, paths[\"gpickle\"])\n",
    "    except Exception:\n",
    "        # Robust fallback: Python pickle\n",
    "        with open(paths[\"gpickle\"], \"wb\") as fh:\n",
    "            pickle.dump(graph_copy, fh, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # JSON node-link\n",
    "    # Explicitly set edges=\"links\" to silence FutureWarning in NetworkX>=3.6\n",
    "    # while preserving current JSON schema.\n",
    "    data = json_graph.node_link_data(graph_copy, edges=\"links\")\n",
    "    paths[\"json\"].write_text(\n",
    "        json.dumps(data, ensure_ascii=False, indent=2),\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "\n",
    "    # CSV of positive-flow edges\n",
    "    with paths[\"csv\"].open(\"w\", newline=\"\", encoding=\"utf-8\") as fh:\n",
    "        writer = csv.writer(fh)\n",
    "        writer.writerow([\"u\", \"v\", \"flow\", \"capacity\", \"weight\"])\n",
    "        for u in flow:\n",
    "            for v, fval in flow[u].items():\n",
    "                if fval > 0 and graph.has_edge(u, v):\n",
    "                    writer.writerow(\n",
    "                        [\n",
    "                            u,\n",
    "                            v,\n",
    "                            int(fval),\n",
    "                            graph[u][v].get(\"capacity\", \"\"),\n",
    "                            graph[u][v].get(\"weight\", \"\"),\n",
    "                        ]\n",
    "                    )\n",
    "\n",
    "    return paths\n",
    "\n",
    "\n",
    "def visualize_assignment_graph(\n",
    "    flow: Dict,\n",
    "    meta: Dict,\n",
    "    export_path: Path,\n",
    "    title: str = \"Assignment\",\n",
    "    max_labels: int = 60,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Create a simple bipartite plot of the assignment graph\n",
    "    (entries -> projects).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    flow : Dict\n",
    "        Flow dictionary as returned by solve_min_cost.\n",
    "    meta : Dict\n",
    "        Metadata dict produced by build_graph_* (must contain 'entries'\n",
    "        and 'unassigned').\n",
    "    export_path : Path\n",
    "        Where to save the PNG figure.\n",
    "    title : str\n",
    "        Figure title.\n",
    "    max_labels : int\n",
    "        Maximum number of node/edge labels to draw (to avoid clutter).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Path\n",
    "        The saved image path.\n",
    "    \"\"\"\n",
    "    h = nx.DiGraph()\n",
    "    entries = meta[\"entries\"]\n",
    "    _unassigned = meta[\"unassigned\"]  # kept for completeness, not used directly\n",
    "\n",
    "    # Left nodes = \"group\" nodes e0, e1, ...\n",
    "    left_nodes: List[str] = []\n",
    "    for i, _ in entries.reset_index(drop=True).iterrows():\n",
    "        u = f\"e{i}\"\n",
    "        h.add_node(u, bipartite=0)\n",
    "        left_nodes.append(u)\n",
    "\n",
    "    # Keep only edges group -> project (ignore _s, _t)\n",
    "    edgelist: List[Tuple[str, str]] = []\n",
    "    right_nodes_set = set()\n",
    "    for u, outs in flow.items():\n",
    "        if not str(u).startswith(\"e\"):\n",
    "            continue\n",
    "        for v, fval in outs.items():\n",
    "            if fval > 0 and v not in (\"_s\", \"_t\"):\n",
    "                h.add_node(v, bipartite=1)\n",
    "                h.add_edge(u, v, weight=int(fval))\n",
    "                edgelist.append((u, v))\n",
    "                right_nodes_set.add(v)\n",
    "\n",
    "    # Bipartite layout positions\n",
    "    pos: Dict[str, Tuple[float, float]] = {}\n",
    "    for idx, u in enumerate(left_nodes):\n",
    "        pos[u] = (0.0, -idx)\n",
    "\n",
    "    right_nodes = sorted(right_nodes_set)\n",
    "    for idx, v in enumerate(right_nodes):\n",
    "        pos[v] = (1.0, -idx)\n",
    "\n",
    "    # Figure size scales with node counts (basic heuristic)\n",
    "    fig_w = max(8.0, len(right_nodes) * 0.25 + 6.0)\n",
    "    fig_h = max(6.0, len(left_nodes) * 0.12 + 4.0)\n",
    "\n",
    "    plt.figure(figsize=(fig_w, fig_h))\n",
    "    nx.draw_networkx_nodes(\n",
    "        h,\n",
    "        pos,\n",
    "        nodelist=left_nodes,\n",
    "        node_shape=\"s\",\n",
    "        node_size=200,\n",
    "        alpha=0.85,\n",
    "    )\n",
    "    nx.draw_networkx_nodes(\n",
    "        h,\n",
    "        pos,\n",
    "        nodelist=right_nodes,\n",
    "        node_shape=\"o\",\n",
    "        node_size=300,\n",
    "        alpha=0.9,\n",
    "    )\n",
    "\n",
    "    # Edge widths proportional to assigned flow\n",
    "    widths = [1 + 2 * h[u][v][\"weight\"] for (u, v) in edgelist]\n",
    "    if edgelist:\n",
    "        nx.draw_networkx_edges(\n",
    "            h,\n",
    "            pos,\n",
    "            edgelist=edgelist,\n",
    "            width=widths,\n",
    "            arrows=False,\n",
    "            alpha=0.5,\n",
    "        )\n",
    "\n",
    "    if len(left_nodes) <= max_labels:\n",
    "        nx.draw_networkx_labels(\n",
    "            h,\n",
    "            pos,\n",
    "            labels={u: u for u in left_nodes},\n",
    "            font_size=8,\n",
    "        )\n",
    "    if len(right_nodes) <= max_labels:\n",
    "        nx.draw_networkx_labels(\n",
    "            h,\n",
    "            pos,\n",
    "            labels={v: v for v in right_nodes},\n",
    "            font_size=9,\n",
    "        )\n",
    "\n",
    "    if edgelist and len(edgelist) <= max_labels:\n",
    "        nx.draw_networkx_edge_labels(\n",
    "            h,\n",
    "            pos,\n",
    "            edge_labels={(u, v): h[u][v][\"weight\"] for (u, v) in edgelist},\n",
    "            font_size=7,\n",
    "        )\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    export_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(export_path, dpi=200)\n",
    "    plt.close()\n",
    "    return export_path\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 9) Exports from an existing main\n",
    "# ================================\n",
    "if __name__ == \"__main__\":\n",
    "    # The following assumes that the functions below are available in the\n",
    "    # current module or imported from your assignment pipeline:\n",
    "    # - resolve_data_dir\n",
    "    # - load_projects_df\n",
    "    # - load_choices_df\n",
    "    # - build_graph_unweighted\n",
    "    # - build_graph_weighted\n",
    "    # - solve_min_cost\n",
    "\n",
    "    # Resolve data directory locally (do not rely on another module's global)\n",
    "    DATA_DIR = resolve_data_dir()\n",
    "    EXPORT_DIR = (DATA_DIR / \"exports\").resolve()\n",
    "\n",
    "    def _pretty_path(path: Path, base: Path) -> str:\n",
    "        \"\"\"\n",
    "        Return a path relative to 'base' if possible; otherwise just the\n",
    "        filename. This avoids leaking absolute directories in console output.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return str(path.relative_to(base))\n",
    "        except Exception:\n",
    "            return path.name\n",
    "\n",
    "    # --- Unweighted: rebuild graph/flow for export ---\n",
    "    prj_df = load_projects_df(DATA_DIR / \"projects.csv\")\n",
    "    ch_df = load_choices_df(\n",
    "        DATA_DIR / \"student-choices.csv\",\n",
    "        prj_df[\"id\"].tolist(),\n",
    "    )\n",
    "    g_unw, meta_unw = build_graph_unweighted(\n",
    "        ch_df,\n",
    "        prj_df,\n",
    "        rank_cost=None,\n",
    "        penalty=None,\n",
    "        unassigned_label=\"__NA__\",\n",
    "    )\n",
    "    flow_unw, _cost_unw = solve_min_cost(g_unw)\n",
    "    paths_unw = export_graph_models(\n",
    "        g_unw,\n",
    "        meta_unw,\n",
    "        flow_unw,\n",
    "        EXPORT_DIR,\n",
    "        prefix=\"unweighted_model\",\n",
    "    )\n",
    "    vis_unw = visualize_assignment_graph(\n",
    "        flow_unw,\n",
    "        meta_unw,\n",
    "        EXPORT_DIR / \"unweighted_assignment.png\",\n",
    "        title=\"Assignment (Unweighted)\",\n",
    "    )\n",
    "\n",
    "    # --- Weighted: rebuild graph/flow for export ---\n",
    "    g_w, meta_w = build_graph_weighted(\n",
    "        ch_df,\n",
    "        prj_df,\n",
    "        penalty=10.0,\n",
    "        unassigned_label=\"__NA__\",\n",
    "    )\n",
    "    flow_w, _cost_w = solve_min_cost(g_w)\n",
    "    paths_w = export_graph_models(\n",
    "        g_w,\n",
    "        meta_w,\n",
    "        flow_w,\n",
    "        EXPORT_DIR,\n",
    "        prefix=\"weighted_model\",\n",
    "    )\n",
    "    vis_w = visualize_assignment_graph(\n",
    "        flow_w,\n",
    "        meta_w,\n",
    "        EXPORT_DIR / \"weighted_assignment.png\",\n",
    "        title=\"Assignment (Weighted)\",\n",
    "    )\n",
    "\n",
    "    print(\"\\nExports (unweighted):\")\n",
    "    for key, path in paths_unw.items():\n",
    "        print(f\"  {key}: {_pretty_path(path, DATA_DIR)}\")\n",
    "    print(\"  viz:\", _pretty_path(vis_unw, DATA_DIR))\n",
    "\n",
    "    print(\"\\nExports (weighted):\")\n",
    "    for key, path in paths_w.items():\n",
    "        print(f\"  {key}: {_pretty_path(path, DATA_DIR)}\")\n",
    "    print(\"  viz:\", _pretty_path(vis_w, DATA_DIR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aacac988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3x3] Done.\n",
      " - Cost (unweighted): 0.0\n",
      " - Cost (weighted)  : 0.8999999999999999\n",
      " - Files written:\n",
      "   • assignment_student_unweighted.csv\n",
      "   • assignment_project_unweighted.csv\n",
      "   • assignment_student_weighted.csv\n",
      "   • assignment_project_weighted.csv\n",
      "   • Exports in: exports\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Minimal batch for the 3 projects / 3 students sample.\n",
    "\n",
    "- Reads ONLY: data/3_sample/3_projects.csv and\n",
    "              data/3_sample/3_student-choices.csv\n",
    "- Runs both variants (unweighted + weighted) WITHOUT renaming/copying files\n",
    "- Writes outputs into data/3_sample/\n",
    "- Exports models and visualizations into data/3_sample/exports\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def run_sample_3x3() -> None:\n",
    "    \"\"\"Run the 3x3 sample batch end-to-end.\"\"\"\n",
    "    base_data_dir = resolve_data_dir()\n",
    "    sample_dir = (base_data_dir / \"3_sample\").resolve()\n",
    "    sample_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # --- Strict inputs ---\n",
    "    src_projects = sample_dir / \"3_projects.csv\"\n",
    "    src_choices = sample_dir / \"3_student-choices.csv\"\n",
    "\n",
    "    if not src_projects.exists() or not src_choices.exists():\n",
    "        raise FileNotFoundError(\n",
    "            \"Missing files in \"\n",
    "            f\"{sample_dir}: \"\n",
    "            f\"{'OK' if src_projects.exists() else 'MISSING: 3_projects.csv'}, \"\n",
    "            f\"{'OK' if src_choices.exists() else 'MISSING: 3_student-choices.csv'}\"\n",
    "        )\n",
    "\n",
    "    # --- Load input data ---\n",
    "    prj_df = load_projects_df(src_projects)\n",
    "    ch_df = load_choices_df(src_choices, prj_df[\"id\"].tolist())\n",
    "\n",
    "    # === UNWEIGHTED variant (ordered / ranks) ===\n",
    "    g_unw, meta_unw = build_graph_unweighted(\n",
    "        entries_df=ch_df,\n",
    "        projects_df=prj_df,\n",
    "        rank_cost=None,\n",
    "        penalty=None,\n",
    "        unassigned_label=\"__NA__\",\n",
    "    )\n",
    "    flow_unw, cost_unw = solve_min_cost(g_unw)\n",
    "\n",
    "    assign_unw = expand_to_individual_rows(flow_unw, meta_unw)\n",
    "    students_unw_df = build_student_df(assign_unw, prj_df, mode=\"unweighted\")\n",
    "    projects_unw_df = build_project_df(\n",
    "        assign_unw,\n",
    "        prj_df,\n",
    "        include_unassigned=True,\n",
    "        unassigned_label=\"__NA__\",\n",
    "    )\n",
    "\n",
    "    out_student_unw = sample_dir / \"assignment_student_unweighted.csv\"\n",
    "    out_project_unw = sample_dir / \"assignment_project_unweighted.csv\"\n",
    "    write_students_csv(students_unw_df, out_student_unw)\n",
    "    write_projects_csv(projects_unw_df, out_project_unw)\n",
    "\n",
    "    # === WEIGHTED variant (explicit weights) ===\n",
    "    g_w, meta_w = build_graph_weighted(\n",
    "        entries_df=ch_df,\n",
    "        projects_df=prj_df,\n",
    "        penalty=10.0,\n",
    "        unassigned_label=\"__NA__\",\n",
    "    )\n",
    "    flow_w, cost_w = solve_min_cost(g_w)\n",
    "\n",
    "    assign_w = expand_to_individual_rows(flow_w, meta_w)\n",
    "    students_w_df = build_student_df(assign_w, prj_df, mode=\"weighted\")\n",
    "    projects_w_df = build_project_df(\n",
    "        assign_w,\n",
    "        prj_df,\n",
    "        include_unassigned=True,\n",
    "        unassigned_label=\"__NA__\",\n",
    "    )\n",
    "\n",
    "    out_student_w = sample_dir / \"assignment_student_weighted.csv\"\n",
    "    out_project_w = sample_dir / \"assignment_project_weighted.csv\"\n",
    "    write_students_csv(students_w_df, out_student_w)\n",
    "    write_projects_csv(projects_w_df, out_project_w)\n",
    "\n",
    "    # --- Graph/model exports ---\n",
    "    export_dir = sample_dir / \"exports\"\n",
    "    export_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    export_graph_models(\n",
    "        g_unw, meta_unw, flow_unw, export_dir, prefix=\"unweighted_model\"\n",
    "    )\n",
    "    visualize_assignment_graph(\n",
    "        flow_unw,\n",
    "        meta_unw,\n",
    "        export_dir / \"unweighted_assignment.png\",\n",
    "        title=\"Assignment (Unweighted)\",\n",
    "    )\n",
    "\n",
    "    export_graph_models(\n",
    "        g_w, meta_w, flow_w, export_dir, prefix=\"weighted_model\"\n",
    "    )\n",
    "    visualize_assignment_graph(\n",
    "        flow_w,\n",
    "        meta_w,\n",
    "        export_dir / \"weighted_assignment.png\",\n",
    "        title=\"Assignment (Weighted)\",\n",
    "    )\n",
    "\n",
    "    # Pretty console output without leaking absolute directories\n",
    "    def _pretty_path(path, base):\n",
    "        try:\n",
    "            return str(path.relative_to(base))\n",
    "        except Exception:\n",
    "            return path.name\n",
    "\n",
    "    print(\"\\n[3x3] Done.\")\n",
    "    print(f\" - Cost (unweighted): {cost_unw}\")\n",
    "    print(f\" - Cost (weighted)  : {cost_w}\")\n",
    "    print(\" - Files written:\")\n",
    "    print(f\"   • {_pretty_path(out_student_unw, sample_dir)}\")\n",
    "    print(f\"   • {_pretty_path(out_project_unw, sample_dir)}\")\n",
    "    print(f\"   • {_pretty_path(out_student_w, sample_dir)}\")\n",
    "    print(f\"   • {_pretty_path(out_project_w, sample_dir)}\")\n",
    "    print(f\"   • Exports in: {_pretty_path(export_dir, sample_dir)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Launch the 3x3 batch when invoked as a script\n",
    "    run_sample_3x3()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
